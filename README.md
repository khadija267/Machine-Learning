# Machine Learning


<!-- Add banner here -->
![Banner](https://miro.medium.com/max/1400/1*c_fiB-YgbnMl6nntYGBMHQ.jpeg)


<!-- Add buttons here -->
###  Machine Learning Solutions in Python

![GitHub release (latest by date including pre-releases)](https://img.shields.io/github/v/release/navendu-pottekkat/awesome-readme?include_prereleases)
![GitHub last commit](https://img.shields.io/github/last-commit/navendu-pottekkat/awesome-readme)
![GitHub issues](https://img.shields.io/github/issues-raw/navendu-pottekkat/awesome-readme)
![GitHub pull requests](https://img.shields.io/github/issues-pr/navendu-pottekkat/awesome-readme)

# Table of Content

# 1. Finding Charity Donors:

Binary classification to detect whether the customer will be a probable donor or not.
Supervised learning using adaboost, SVC and logistics regression.
The dataset for this project originates from the UCI Machine Learning Repository. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid". 
Algorithms metrics in terms of time complexity,f1score and accuracy:<br>
![alt text](https://github.com/khadija267/Machine-Learning/blob/main/images/donors.png?raw=true)


## 2. Diabetes Case Study:
We looked at 768 people in this case study to see if we could predict diabetes. With little under 35% of patients having diabetes, there was a reasonable amount of class imbalance. There were no gaps in the data, and preliminary analysis revealed that it would be difficult to distinguish between diabetic and non-diabetic patients. We used adaboost, random forest and SVC. The champion model was adaboost with F1 score of 0.65. 
## 3. Boston Housing Regression:
Using adaboost, decision tree, random forest and linear regression, the random forest was the champion model with a high r2 score of 0.81, and very low MSE and MAE.
![alt text](https://github.com/khadija267/Machine-Learning/blob/main/images/3.png?raw=true)




## 1. Titanic Survival:



